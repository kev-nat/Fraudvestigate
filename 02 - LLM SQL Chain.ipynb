{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8562824",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934eca7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 701, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 469, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 379, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 899, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 471, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 632, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_15700\\909135856.py\", line 10, in <module>\n",
      "    from langchain_openai import ChatOpenAI\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_openai\\__init__.py\", line 3, in <module>\n",
      "    from langchain_openai.chat_models import AzureChatOpenAI, ChatOpenAI\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_openai\\chat_models\\__init__.py\", line 3, in <module>\n",
      "    from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_openai\\chat_models\\azure.py\", line 11, in <module>\n",
      "    from langchain_core.language_models import LanguageModelInput\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_core\\language_models\\__init__.py\", line 110, in __getattr__\n",
      "    result = import_attr(attr_name, module_name, __spec__.parent)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_core\\_import_utils.py\", line 35, in import_attr\n",
      "    module = import_module(f\".{module_name}\", package=package)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\langchain_core\\language_models\\base.py\", line 41, in <module>\n",
      "    from transformers import GPT2TokenizerFast  # type: ignore[import-not-found]\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\utils\\__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 51, in <module>\n",
      "    import torch\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\Users\\kevin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13eda708",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_uri = \"postgresql://postgres:111@localhost:5432/postgres\"\n",
    "db = SQLDatabase.from_uri(db_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3c6ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Names: ['TRXFraud']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_15700\\3357723900.py:1: LangChainDeprecationWarning: The method `SQLDatabase.get_table_names` was deprecated in langchain-community 0.0.1 and will be removed in 1.0. Use `get_usable_table_names` instead.\n",
      "  print(\"Table Names:\", db.get_table_names())\n"
     ]
    }
   ],
   "source": [
    "print(\"Table Names:\", db.get_table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f28953",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "judge_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "eval_embeddings = OllamaEmbeddings(model=\"qwen3-embedding:0.6b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f4191",
   "metadata": {},
   "source": [
    "## SQL Chain (Database Schema + LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8c49205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: LLM doesn't know what categories exist (ex: \"food_dining\", \"gas_transport\").\n",
    "# Solution: Fetch them dynamically so the LLM writes accurate WHERE clauses.\n",
    "\n",
    "def get_fraud_context(_):\n",
    "    # Get standard schema\n",
    "    schema = db.get_table_info()\n",
    "    \n",
    "    categories = db.run('SELECT DISTINCT category FROM \"TRXFraud\" LIMIT 20;')\n",
    "    \n",
    "    return f\"\"\"\n",
    "    Schema:\n",
    "    {schema}\n",
    "    \n",
    "    Available 'category' values: {categories}\n",
    "    \n",
    "    Important Notes:\n",
    "    - Table name is 'TRXFraud'.\n",
    "    - 'is_fraud' is 1 for fraud, 0 for legitimate.\n",
    "    - 'transaction_amount' is the column for money/value.\n",
    "    - 'trans_date_trans_time' is the timestamp.\n",
    "    - When asked for \"Fluctuation\", aggregate by DATE_TRUNC('month', trans_date_trans_time) or 'day'.\n",
    "    - **CRITICAL: When using GROUP BY, always SELECT the COUNT(*) or SUM() column so the values are visible.**\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f60858bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Generation Prompt\n",
    "sql_template = \"\"\"You are a PostgreSQL expert.\n",
    "Given an input question, create a syntactically correct PostgreSQL query to run.\n",
    "\n",
    "**CRITICAL RULES:**\n",
    "1. **Table Name:** You MUST wrap the table name in double quotes: \"TRXFraud\".\n",
    "2. **Schema Only:** Use ONLY the columns listed in the schema below.\n",
    "3. **No Hallucinations:** If the user asks for data that is NOT in the schema (e.g. \"cross-border\", \"country\", \"merchant_state\"), do NOT guess. Return exactly:\n",
    "   SELECT 'N/A' AS result;\n",
    "4. **Fraud Logic:** 'is_fraud' is 1 (fraud) and 0 (legitimate).\n",
    "\n",
    "**Schema:**\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "\n",
    "sql_prompt = ChatPromptTemplate.from_template(sql_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ca0917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Chain\n",
    "sql_chain = (\n",
    "    RunnablePassthrough.assign(schema=get_fraud_context)\n",
    "    | sql_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8773d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution Function\n",
    "def run_query(query):\n",
    "    # Clean up common LLM formatting mistakes before running\n",
    "    cleaned_query = query.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "    print(f\"DEBUG - Generated SQL: {cleaned_query}\")\n",
    "    try:\n",
    "        return db.run(cleaned_query)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958df5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Response Prompt\n",
    "response_template = \"\"\"Based on the question, the SQL query generated, and the database response, write a natural language answer.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\n",
    "\n",
    "If the response is empty, say \"I couldn't find any data matching that request.\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(response_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8a5cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Pipeline\n",
    "full_chain = (\n",
    "    RunnablePassthrough.assign(query=sql_chain).assign(\n",
    "        schema=get_fraud_context,\n",
    "        response=lambda vars: run_query(vars[\"query\"])\n",
    "    )\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986c368",
   "metadata": {},
   "source": [
    "## Sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cadcd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Generated SQL: SELECT COUNT(*) AS total_transactions FROM \"TRXFraud\";\n",
      "There are a total of 1,852,394 transactions in the database.\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Basic count (Sanity Check)\n",
    "response = full_chain.invoke({\"question\": \"How many total transactions are in the database?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9e0795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Generated SQL: SELECT DATE_TRUNC('month', trans_date_trans_time) AS month, COUNT(*) AS fraud_count\n",
      "FROM \"TRXFraud\"\n",
      "WHERE is_fraud = 1\n",
      "GROUP BY month\n",
      "ORDER BY month;\n",
      "Here is the monthly count of fraud cases over the specified period:\n",
      "\n",
      "- January 2019: 506 cases\n",
      "- February 2019: 517 cases\n",
      "- March 2019: 494 cases\n",
      "- April 2019: 376 cases\n",
      "- May 2019: 408 cases\n",
      "- June 2019: 354 cases\n",
      "- July 2019: 331 cases\n",
      "- August 2019: 382 cases\n",
      "- September 2019: 418 cases\n",
      "- October 2019: 454 cases\n",
      "- November 2019: 388 cases\n",
      "- December 2019: 592 cases\n",
      "- January 2020: 343 cases\n",
      "- February 2020: 336 cases\n",
      "- March 2020: 444 cases\n",
      "- April 2020: 302 cases\n",
      "- May 2020: 527 cases\n",
      "- June 2020: 467 cases\n",
      "- July 2020: 321 cases\n",
      "- August 2020: 415 cases\n",
      "- September 2020: 340 cases\n",
      "- October 2020: 384 cases\n",
      "- November 2020: 294 cases\n",
      "- December 2020: 258 cases\n",
      "\n",
      "This data reflects the number of fraud cases reported each month from January 2019 to December 2020.\n"
     ]
    }
   ],
   "source": [
    "# Test 2: The \"Fluctuation\" Question\n",
    "response = full_chain.invoke({\"question\": \"Show me the monthly count of fraud cases over the period.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea983eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Generated SQL: SELECT category, COUNT(*) AS fraud_count\n",
      "FROM \"TRXFraud\"\n",
      "WHERE is_fraud = 1\n",
      "GROUP BY category\n",
      "ORDER BY fraud_count DESC;\n",
      "The merchant categories with the highest number of fraud transactions are as follows: \n",
      "\n",
      "1. Grocery (Point of Sale) - 2,228 transactions\n",
      "2. Shopping (Online) - 2,219 transactions\n",
      "3. Miscellaneous (Online) - 1,182 transactions\n",
      "4. Shopping (Point of Sale) - 1,056 transactions\n",
      "5. Gas and Transportation - 772 transactions\n",
      "6. Miscellaneous (Point of Sale) - 322 transactions\n",
      "7. Kids and Pets - 304 transactions\n",
      "8. Entertainment - 292 transactions\n",
      "9. Personal Care - 290 transactions\n",
      "10. Home - 265 transactions\n",
      "11. Food and Dining - 205 transactions\n",
      "12. Health and Fitness - 185 transactions\n",
      "13. Grocery (Online) - 175 transactions\n",
      "14. Travel - 156 transactions\n",
      "\n",
      "These categories represent the areas with the most reported fraud incidents.\n"
     ]
    }
   ],
   "source": [
    "# Test 3: The \"Category\" Question (Challenge Question 2)\n",
    "response = full_chain.invoke({\"question\": \"Which merchant categories have the highest number of fraud transactions?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "874f25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Generated SQL: SELECT 'N/A' AS result;\n",
      "I couldn't find any data matching that request.\n"
     ]
    }
   ],
   "source": [
    "response = full_chain.invoke({\"question\": \"What are the primary methods by which credit card fraud is committed?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4d35088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Generated SQL: SELECT \n",
      "    DATE_TRUNC('day', trans_date_trans_time) AS period,\n",
      "    SUM(is_fraud) AS total_fraud,\n",
      "    COUNT(*) AS total_transactions,\n",
      "    (SUM(is_fraud)::decimal / COUNT(*)) * 100 AS fraud_rate\n",
      "FROM \n",
      "    \"TRXFraud\"\n",
      "WHERE \n",
      "    trans_date_trans_time >= NOW() - INTERVAL '2 years'\n",
      "GROUP BY \n",
      "    period\n",
      "ORDER BY \n",
      "    period;\n",
      "I couldn't find any data matching that request.\n"
     ]
    }
   ],
   "source": [
    "response = full_chain.invoke({\"question\": \"How does the daily or monthly fraud rate fluctuate over the two-year period?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "682afa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG - Generated SQL: SELECT 'N/A' AS result;\n",
      "I couldn't find any data matching that request.\n"
     ]
    }
   ],
   "source": [
    "response = full_chain.invoke({\"question\": \"What share of total card fraud value in H1 2020 was due to cross-border transactions?\"})\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15cf97b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf5a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"Metric: Semantic Similarity (Relevance)\"\"\"\n",
    "    if not text1 or not text2: return 0.0\n",
    "    \n",
    "    vec1 = eval_embeddings.embed_query(text1)\n",
    "    vec2 = eval_embeddings.embed_query(text2)\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "994d6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_judge_score(question, answer, sql):\n",
    "    \"\"\"Metric: LLM Judge Score (Logic & Accuracy)\"\"\"\n",
    "    judge_prompt = f\"\"\"\n",
    "    Act as an impartial judge. Evaluate the quality of the answer based on the SQL query and the User Question.\n",
    "    \n",
    "    Question: {question}\n",
    "    Generated SQL: {sql}\n",
    "    Final Answer: {answer}\n",
    "    \n",
    "    Give a score from 1 to 10 where:\n",
    "    1 = Completely wrong, SQL error, or irrelevant.\n",
    "    5 = Partially correct but missed nuances.\n",
    "    10 = Perfect logic and accurate answer.\n",
    "    \n",
    "    Return JSON ONLY: {{\"score\": <number>}}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = judge_llm.invoke(judge_prompt).content\n",
    "        \n",
    "        # Robust parsing: find the first { and last }\n",
    "        start = response.find(\"{\")\n",
    "        end = response.rfind(\"}\") + 1\n",
    "        if start == -1 or end == -1: return 0\n",
    "        \n",
    "        data = json.loads(response[start:end])\n",
    "        return data.get(\"score\", 0)\n",
    "    except Exception as e:\n",
    "        print(f\"Judge Error: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "732e550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(test_questions):\n",
    "    results = []\n",
    "    print(f\"Evaluation on {len(test_questions)} test cases...\\n\")\n",
    "    \n",
    "    for i, q in enumerate(test_questions):\n",
    "        print(f\"Evaluating Q{i+1}: {q}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Generate SQL (Intermediate Step)\n",
    "            generated_sql = sql_chain.invoke({\"question\": q})\n",
    "            \n",
    "            # Generate Final Answer\n",
    "            final_answer = full_chain.invoke({\"question\": q})\n",
    "            \n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "            \n",
    "            # Calculate Metrics\n",
    "            # Success: Did it produce an error message?\n",
    "            success = 1 if \"Error\" not in final_answer and \"couldn't find\" not in final_answer else 0\n",
    "            \n",
    "            # Relevance: Is the answer topically related to the question?\n",
    "            sim_score = calculate_similarity(q, final_answer)\n",
    "            \n",
    "            # Logic: Did the SQL make sense?\n",
    "            quality_score = llm_judge_score(q, final_answer, generated_sql)\n",
    "            \n",
    "            results.append({\n",
    "                \"Question\": q,\n",
    "                \"Latency (s)\": round(latency, 2),\n",
    "                \"Success\": success,\n",
    "                \"Semantic Sim\": round(sim_score, 2),\n",
    "                \"Judge Score (1-10)\": quality_score,\n",
    "                \"Answer Snippet\": final_answer[:100] + \"...\"\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed on Q{i+1}: {e}\")\n",
    "            results.append({\n",
    "                \"Question\": q, \n",
    "                \"Success\": 0, \n",
    "                \"Error\": str(e),\n",
    "                \"Latency (s)\": 0,\n",
    "                \"Semantic Sim\": 0,\n",
    "                \"Judge Score (1-10)\": 0\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4102129",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [\n",
    "    # Basic Count\n",
    "    \"How many total transactions are in the database?\",\n",
    "    \n",
    "    # Filtering (Fraud)\n",
    "    \"How many transactions are flagged as fraud?\",\n",
    "    \n",
    "    # Summation (Money)\n",
    "    \"What is the total dollar value of all fraud transactions?\",\n",
    "    \n",
    "    # Grouping (Time)\n",
    "    \"Show me the fraud count grouped by month.\",\n",
    "    \n",
    "    # Ranking (Max)\n",
    "    \"Which category has the highest number of fraud cases?\",\n",
    "    \n",
    "    # Specific Filter\n",
    "    \"How many transactions occurred in 2020?\",\n",
    "    \n",
    "    # Complex (Fraud Rate)\n",
    "    \"What is the average amount of a fraud transaction?\",\n",
    "    \n",
    "    # Specific Category\n",
    "    \"How many fraud cases happened in the 'grocery_pos' category?\",\n",
    "    \n",
    "    # Comparison\n",
    "    \"Which year had more fraud cases, 2019 or 2020?\",\n",
    "    \n",
    "    # Negative/Hallucination Check\n",
    "    \"How many transactions were from Canada?\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ff6ea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on 10 test cases...\n",
      "\n",
      "Evaluating Q1: How many total transactions are in the database?...\n",
      "DEBUG - Generated SQL: SELECT COUNT(*) AS total_transactions FROM \"TRXFraud\";\n",
      "Evaluating Q2: How many transactions are flagged as fraud?...\n",
      "DEBUG - Generated SQL: SELECT COUNT(*) AS fraud_count \n",
      "FROM \"TRXFraud\" \n",
      "WHERE is_fraud = 1;\n",
      "Evaluating Q3: What is the total dollar value of all fraud transactions?...\n",
      "DEBUG - Generated SQL: SELECT SUM(transaction_amount) AS total_fraud_amount\n",
      "FROM \"TRXFraud\"\n",
      "WHERE is_fraud = 1;\n",
      "Evaluating Q4: Show me the fraud count grouped by month....\n",
      "DEBUG - Generated SQL: SELECT DATE_TRUNC('month', trans_date_trans_time) AS month, COUNT(*) AS fraud_count\n",
      "FROM \"TRXFraud\"\n",
      "WHERE is_fraud = 1\n",
      "GROUP BY month\n",
      "ORDER BY month;\n",
      "Evaluating Q5: Which category has the highest number of fraud cases?...\n",
      "DEBUG - Generated SQL: SELECT category, COUNT(*) AS fraud_count\n",
      "FROM \"TRXFraud\"\n",
      "WHERE is_fraud = 1\n",
      "GROUP BY category\n",
      "ORDER BY fraud_count DESC\n",
      "LIMIT 1;\n",
      "Evaluating Q6: How many transactions occurred in 2020?...\n",
      "DEBUG - Generated SQL: SELECT COUNT(*) AS transaction_count\n",
      "FROM \"TRXFraud\"\n",
      "WHERE trans_date_trans_time >= '2020-01-01' AND trans_date_trans_time < '2021-01-01';\n",
      "Evaluating Q7: What is the average amount of a fraud transaction?...\n",
      "DEBUG - Generated SQL: SELECT AVG(transaction_amount) AS average_fraud_amount\n",
      "FROM \"TRXFraud\"\n",
      "WHERE is_fraud = 1;\n",
      "Evaluating Q8: How many fraud cases happened in the 'grocery_pos' category?...\n",
      "DEBUG - Generated SQL: SELECT COUNT(*) AS fraud_cases\n",
      "FROM \"TRXFraud\"\n",
      "WHERE category = 'grocery_pos' AND is_fraud = 1;\n",
      "Evaluating Q9: Which year had more fraud cases, 2019 or 2020?...\n",
      "DEBUG - Generated SQL: SELECT \n",
      "    EXTRACT(YEAR FROM trans_date_trans_time) AS year, \n",
      "    COUNT(*) AS fraud_cases\n",
      "FROM \n",
      "    \"TRXFraud\"\n",
      "WHERE \n",
      "    is_fraud = 1 AND \n",
      "    EXTRACT(YEAR FROM trans_date_trans_time) IN (2019, 2020)\n",
      "GROUP BY \n",
      "    year\n",
      "ORDER BY \n",
      "    year;\n",
      "Evaluating Q10: How many transactions were from Canada?...\n",
      "DEBUG - Generated SQL: SELECT 'N/A' AS result;\n",
      "EVALUATION RESULTS\n",
      "Success Rate:    90%\n",
      "Avg Latency:     5.84s\n",
      "Avg Judge Score: 9.1/10\n",
      "Avg Similarity:  0.80\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Latency (s)</th>\n",
       "      <th>Success</th>\n",
       "      <th>Semantic Sim</th>\n",
       "      <th>Judge Score (1-10)</th>\n",
       "      <th>Answer Snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many total transactions are in the database?</td>\n",
       "      <td>6.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.86</td>\n",
       "      <td>10</td>\n",
       "      <td>There are a total of 1,852,394 transactions in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many transactions are flagged as fraud?</td>\n",
       "      <td>6.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>10</td>\n",
       "      <td>There are 9,651 transactions that are flagged ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the total dollar value of all fraud tr...</td>\n",
       "      <td>4.54</td>\n",
       "      <td>1</td>\n",
       "      <td>0.89</td>\n",
       "      <td>10</td>\n",
       "      <td>The total dollar value of all fraud transactio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Show me the fraud count grouped by month.</td>\n",
       "      <td>10.76</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "      <td>10</td>\n",
       "      <td>The fraud count grouped by month is as follows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which category has the highest number of fraud...</td>\n",
       "      <td>5.21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>10</td>\n",
       "      <td>The category with the highest number of fraud ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How many transactions occurred in 2020?</td>\n",
       "      <td>5.08</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>10</td>\n",
       "      <td>In 2020, there were a total of 927,544 transac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the average amount of a fraud transact...</td>\n",
       "      <td>4.73</td>\n",
       "      <td>1</td>\n",
       "      <td>0.89</td>\n",
       "      <td>10</td>\n",
       "      <td>The average amount of a fraud transaction is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many fraud cases happened in the 'grocery_...</td>\n",
       "      <td>5.53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>10</td>\n",
       "      <td>There were 2,228 fraud cases in the 'grocery_p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which year had more fraud cases, 2019 or 2020?</td>\n",
       "      <td>6.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.83</td>\n",
       "      <td>10</td>\n",
       "      <td>In 2019, there were 5,220 fraud cases, while i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How many transactions were from Canada?</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1</td>\n",
       "      <td>I couldn't find any data matching that request...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  Latency (s)  Success  \\\n",
       "0   How many total transactions are in the database?         6.13        1   \n",
       "1        How many transactions are flagged as fraud?         6.02        1   \n",
       "2  What is the total dollar value of all fraud tr...         4.54        1   \n",
       "3          Show me the fraud count grouped by month.        10.76        1   \n",
       "4  Which category has the highest number of fraud...         5.21        1   \n",
       "5            How many transactions occurred in 2020?         5.08        1   \n",
       "6  What is the average amount of a fraud transact...         4.73        1   \n",
       "7  How many fraud cases happened in the 'grocery_...         5.53        1   \n",
       "8     Which year had more fraud cases, 2019 or 2020?         6.82        1   \n",
       "9            How many transactions were from Canada?         3.58        0   \n",
       "\n",
       "   Semantic Sim  Judge Score (1-10)  \\\n",
       "0          0.86                  10   \n",
       "1          0.87                  10   \n",
       "2          0.89                  10   \n",
       "3          0.68                  10   \n",
       "4          0.82                  10   \n",
       "5          0.90                  10   \n",
       "6          0.89                  10   \n",
       "7          0.93                  10   \n",
       "8          0.83                  10   \n",
       "9          0.36                   1   \n",
       "\n",
       "                                      Answer Snippet  \n",
       "0  There are a total of 1,852,394 transactions in...  \n",
       "1  There are 9,651 transactions that are flagged ...  \n",
       "2  The total dollar value of all fraud transactio...  \n",
       "3  The fraud count grouped by month is as follows...  \n",
       "4  The category with the highest number of fraud ...  \n",
       "5  In 2020, there were a total of 927,544 transac...  \n",
       "6  The average amount of a fraud transaction is a...  \n",
       "7  There were 2,228 fraud cases in the 'grocery_p...  \n",
       "8  In 2019, there were 5,220 fraud cases, while i...  \n",
       "9  I couldn't find any data matching that request...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = run_evaluation(test_set)\n",
    "\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(f\"Success Rate:    {df_results['Success'].mean():.0%}\")\n",
    "print(f\"Avg Latency:     {df_results['Latency (s)'].mean():.2f}s\")\n",
    "print(f\"Avg Judge Score: {df_results['Judge Score (1-10)'].mean():.1f}/10\")\n",
    "print(f\"Avg Similarity:  {df_results['Semantic Sim'].mean():.2f}\")\n",
    "\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
